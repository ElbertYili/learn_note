## 基本概念

### Softmax 回归模型

​	分类模型，用于2个以上类型的分类

### 张量

​	[tensor](https://www.zhihu.com/question/20695804)

### 交叉熵

​	熵的本质是香农信息量(
$$
\log 1/p)的期望。
$$
​	现有关于样本集的2个概率分布p和q，其中p为真实分布，q非真实分布。按照真实分布p来衡量识别一个样本的所需要的编码长度的期望(即平均编码长度)为：
$$
H(p)=\sum p(i)*\log (1/p(i))
$$
​	如果使用错误分布q来表示来自真实分布p的平均编码长度，则应该是：
$$
H(p,q)=\sum p(i)*\log (1/q(i))
$$
​	因为用q来编码的样本来自分布p，所以期望H(p,q)中概率是p(i)。H(p,q)我们称之为“交叉熵”。

​	比如含有4个字母(A,B,C,D)的数据集中，真实分布p=(1/2, 1/2, 0, 0)，即A和B出现的概率均为1/2，C和D出现的概率都为0。计算H(p)为1，即只需要1位编码即可识别A和B。如果使用分布Q=(1/4, 1/4, 1/4, 1/4)来编码则得到H(p,q)=2，即需要2位编码来识别A和B(当然还有C和D，尽管C和D并不会出现，因为真实分布p中C和D出现的概率为0，这里就钦定概率为0的事件不会发生啦)。