{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基本概念\n",
    "\n",
    "1. 输入层、隐藏层、输出层\n",
    "   隐藏层决定网络的深度\n",
    "   每一层都由若干神经元构成：  Wx + B ——> 激励函数， 通过激励函数决定是否激活\n",
    "   运算得到Cost(Loss)，在根据算法改变W和B，调整神经元的兴奋状态\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 定义一个神经网络\n",
    "class Network(object):\n",
    "    # sizes : 每层神经元的个数，一个array\n",
    "    def __init__(self, sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        #除了输入层每一个神经元都需要一个Biase\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        # 每个神经元的链接都相当于一个weight，包括了输入、隐藏和输出\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[: -1], sizes[1:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sizess = [2, 3, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.24131356 -1.10329128]\n",
      " [ 1.23876205 -1.69251916]\n",
      " [-0.5074485  -0.56546868]]\n",
      "[[ 0.53427166 -1.57114284  0.95241794]]\n"
     ]
    }
   ],
   "source": [
    "for x, y in zip(sizess[: -1], sizess[1:]):\n",
    "    print np.random.randn(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = Network(sizess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[array([[-0.56017615, -1.96361967],\n",
      "       [-0.08118446, -1.51244957],\n",
      "       [ 0.67825462, -0.246522  ]]), array([[-1.67766405, -0.83679415,  0.9066414 ]])]\n",
      "[array([[ 0.22912532],\n",
      "       [ 2.01787455],\n",
      "       [ 0.1957602 ]]), array([[-1.84991801]])]\n"
     ]
    }
   ],
   "source": [
    "print net.num_layers\n",
    "print net.weights\n",
    "print net.biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义激励函数\n",
    "def feedforward(self, a):\n",
    "    for b, w in zip(self.biases, self.weights):\n",
    "        a = sigmoid(np.dot(w, a) + b)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfiting\n",
    "减少的办法：\n",
    "1. 减少神经网络的复杂度，但是这神经网络的规模和深度能够带来更强的学习能力\n",
    "\n",
    "2. Regularization：\n",
    "    * L2(weight descent) regularization（在Cost计算上加上权重计算）       \n",
    "       **Regularized网络更鼓励小的权重**，小的权重的情况下，X一些随机的变化不会对神经网络的模型造成太大影响，所以更小可能受到数据局部噪音的影响   \n",
    "       更倾向学习到一些简单的模型\n",
    "    * L1 regularization    \n",
    "        都是减小权重    \n",
    "        L1减少一个常量    \n",
    "    * Dropout    \n",
    "        不是针对Cost， 而是针对神经网络本身的结构进行改变    \n",
    "        *不止能减少overfitting，还能提高一定的准确度accuracy*\n",
    "    \n",
    "3. 增加训练数据集的量\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 神经网络参数(hyper-parameters)选择\n",
    "* 包括：学习率（learning rate）、Regularization parameter、mini-batch等\n",
    "* 方法：1、手工挑；2、grid search\n",
    "* 激励函数选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmod训练神经网络中的Vanishing & Gradient问题\n",
    "\n",
    "    越靠近输出层，学习更快；越靠近输入层，学习越慢\n",
    "  #### 解决方式：\n",
    "     * 初始化比较大的权重\n",
    "     * 初始化b， 使激励函数的导数不要太小\n",
    "     \n",
    "    根本在于后面层的梯度是前面层的乘积，所以神经网络不稳定\n",
    "    \n",
    "   **使用ReLu代替Sigmod来训练神经网络**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN(Convolution NeuralNetwork) 算法\n",
    "* local receipt field\n",
    "       共享权重和偏量    \n",
    "       feature map，多个feature map对应多个隐藏层(通常使用大于3个的)\n",
    "       用ReLu代替sigmod\n",
    "       \n",
    "* Pooling    \n",
    "       Max-Pooling\n",
    "       L2 Pooling，平方和开方\n",
    "\n",
    "* Backpropagation, gradient descent,  Fully Connect\n",
    "\n",
    "* CNN本身的convolution层对overfitting有防止作用，所以可以只对Fully层做dropout"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
